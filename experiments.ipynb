{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/phenaki-pytorch/blob/luthando-contribution/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/LuthandoMaqondo/phenaki-pytorch.git\n",
    "    %cd /content/phenaki-pytorch\n",
    "    !git checkout luthando-contribution\n",
    "    !pip install -r requirements.txt\n",
    "    # !pip install phenaki-pytorch\n",
    "    \n",
    "# !pip install git+https://xxxxxxxxxxxx@github.com/AppimateSA/AutoVisual.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AZURE_BLOB_STORAGE_CONN_STR\"] = \"DefaultEndpointsProtocol=https;AccountName=appimate1storage;AccountKey=nM8FPc0H/suHN/bBo7O3LUUFpXPGruvpyUTYOXdjf0UXw9P2snz4+OyIVaykCz+WsLu7n6FLbehM+AStCrLjsA==;EndpointSuffix=core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autovisual import DatasetConfig, VideoCustomDataset\n",
    "# dataset_config_args = {\n",
    "#     'refreshData': True,\n",
    "#     # 'useCLIP': \"openai/clip-vit-large-patch14\", # Use the pretrained CLIP model for handling ALL Text inputs.\n",
    "#     'structure': 'text_video_pair',\n",
    "#     'tokenize_text': False,\n",
    "#     # 'data_folder': f'.{WORKING_DIR}/datasets/Appimate',\n",
    "#     'data_folder': 'https://appimate1storage.blob.core.windows.net/datasets/Appimate',\n",
    "#     'data_json': \"dataset.json\",\n",
    "#     'data_points': None, # None\n",
    "\n",
    "#     'max_text_length': 77,\n",
    "#     'max_num_frames': 6,\n",
    "#     'resolution': 256,\n",
    "#     'num_channels': 1, \n",
    "#     'normalize': True,\n",
    "#     'scale_to': 0,#0.5,\n",
    "#     'has_start_end_token': True,\n",
    "\n",
    "#     'frame_rate': 2,\n",
    "#     'frame_rate_ratio': 0.01,\n",
    "#     'output_format': 'TCHW'\n",
    "# }\n",
    "# datasetConfig = DatasetConfig(**dataset_config_args, train=True)\n",
    "# custom_dataset = VideoCustomDataset(datasetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockTextVideoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        length = 100,\n",
    "        image_size = 256,\n",
    "        num_frames = 17\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.len = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx): # Video data of shape: CTHW\n",
    "        video = torch.randn(3, self.num_frames, self.image_size, self.image_size)\n",
    "        caption = f'video caption {idx}'\n",
    "        return video, caption\n",
    "\n",
    "mock_dataset = MockTextVideoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ConcatDataset([\n",
    "    mock_dataset,\n",
    "    # custom_dataset\n",
    "])\n",
    "\n",
    "# train_len = int(len(full_dataset) * (datasetConfig.train_split) )\n",
    "train_len = int(len(full_dataset) * (.9) )\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_len, len(full_dataset)- train_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the C-ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 494 samples and validating with randomly splitted 26 samples\n"
     ]
    }
   ],
   "source": [
    "from phenaki_pytorch import CViViT, CViViTTrainer\n",
    "\n",
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = (256, 256),\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").to(device)\n",
    "\n",
    "data_folder = os.path.expanduser(f\"{WORKING_DIR}/datasets/Appimate/train\") if IN_COLAB else os.path.expanduser(f\"~/.cache/datasets\")\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder = data_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 4,\n",
    "    train_on_images = False,  # you can train on images first, before fine tuning on video, for sample efficiency\n",
    "    use_ema = True,          # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "\n",
    "    save_results_every = 100,\n",
    "    save_model_every = 100,\n",
    "    num_train_steps = 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: vae loss: 19032.031982421875 - discr loss: 11.95794153213501\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: vae loss: 20086.9287109375 - discr loss: 10.997035503387451\n",
      "2: vae loss: 25521.232421875 - discr loss: 10.768080949783325\n",
      "3: vae loss: 27216.9951171875 - discr loss: 9.937973260879517\n",
      "4: vae loss: 18230.06396484375 - discr loss: 20.128325939178467\n",
      "5: vae loss: 22421.19140625 - discr loss: 8.170398712158203\n",
      "6: vae loss: 17686.709228515625 - discr loss: 9.332286357879639\n",
      "7: vae loss: 16220.058837890625 - discr loss: 11.962282180786133\n",
      "8: vae loss: 18154.435546875 - discr loss: 9.596046209335327\n",
      "9: vae loss: 12768.8271484375 - discr loss: 10.142586469650269\n",
      "10: vae loss: 18826.08056640625 - discr loss: 10.69494104385376\n",
      "11: vae loss: 17306.86376953125 - discr loss: 10.83767557144165\n",
      "12: vae loss: 24295.339599609375 - discr loss: 10.943338871002197\n",
      "13: vae loss: 17165.032958984375 - discr loss: 11.077337980270386\n",
      "14: vae loss: 18728.271606445312 - discr loss: 11.163064002990723\n",
      "15: vae loss: 19341.151611328125 - discr loss: 11.163592338562012\n",
      "16: vae loss: 23454.35107421875 - discr loss: 11.077286005020142\n",
      "17: vae loss: 17800.09375 - discr loss: 10.962637424468994\n",
      "18: vae loss: 20571.93603515625 - discr loss: 10.8020761013031\n",
      "19: vae loss: 15071.403564453125 - discr loss: 10.689449548721313\n",
      "20: vae loss: 17212.970703125 - discr loss: 18.341238021850586\n",
      "21: vae loss: 23409.60400390625 - discr loss: 10.635859727859497\n",
      "22: vae loss: 23877.7275390625 - discr loss: 10.522872924804688\n",
      "23: vae loss: 17789.30908203125 - discr loss: 10.166977167129517\n",
      "24: vae loss: 19372.3486328125 - discr loss: 9.14944314956665\n",
      "25: vae loss: 14891.91552734375 - discr loss: 9.105602979660034\n",
      "26: vae loss: 21171.444580078125 - discr loss: 9.046351909637451\n",
      "27: vae loss: 24188.4716796875 - discr loss: 8.689824104309082\n",
      "28: vae loss: 22010.171875 - discr loss: 8.166142225265503\n",
      "29: vae loss: 23950.32470703125 - discr loss: 7.185774803161621\n",
      "30: vae loss: 25547.291259765625 - discr loss: 6.972406268119812\n",
      "31: vae loss: 19988.089111328125 - discr loss: 4.265973389148712\n",
      "32: vae loss: 15936.756103515625 - discr loss: 1.5300414562225342\n",
      "33: vae loss: 24877.417236328125 - discr loss: 0.762959785759449\n",
      "34: vae loss: 15239.96923828125 - discr loss: 3.8018878698349\n",
      "35: vae loss: 23764.73583984375 - discr loss: 1.0392860174179077\n",
      "36: vae loss: 20240.544677734375 - discr loss: 1.3865890502929688\n",
      "37: vae loss: 19289.7001953125 - discr loss: 3.0917553901672363\n",
      "38: vae loss: 15227.421630859375 - discr loss: 3.766339361667633\n",
      "39: vae loss: 22726.64306640625 - discr loss: 13.634884238243103\n",
      "40: vae loss: 22517.3818359375 - discr loss: 10.283598184585571\n",
      "41: vae loss: 17247.5009765625 - discr loss: 15.13542103767395\n",
      "42: vae loss: 18090.989013671875 - discr loss: 13.05483102798462\n",
      "43: vae loss: 15986.927001953125 - discr loss: 7.694958925247192\n",
      "44: vae loss: 20115.080810546875 - discr loss: 14.835636496543884\n",
      "45: vae loss: 20007.973388671875 - discr loss: 10.949299335479736\n",
      "46: vae loss: 18877.843994140625 - discr loss: 13.013164520263672\n",
      "47: vae loss: 19920.020751953125 - discr loss: 14.022579193115234\n",
      "48: vae loss: 19801.566650390625 - discr loss: 12.420209407806396\n",
      "49: vae loss: 13306.2958984375 - discr loss: 10.222530364990234\n",
      "50: vae loss: 19200.76171875 - discr loss: 9.775748491287231\n",
      "51: vae loss: 16195.02880859375 - discr loss: 10.013513803482056\n",
      "52: vae loss: 23002.42138671875 - discr loss: 9.17886507511139\n",
      "53: vae loss: 15344.60498046875 - discr loss: 9.87751317024231\n",
      "54: vae loss: 20676.42626953125 - discr loss: 10.064264059066772\n",
      "55: vae loss: 22998.71435546875 - discr loss: 10.31466269493103\n",
      "56: vae loss: 18106.311279296875 - discr loss: 9.97194242477417\n",
      "57: vae loss: 13113.58349609375 - discr loss: 10.092807054519653\n",
      "58: vae loss: 17543.09228515625 - discr loss: 10.621721506118774\n",
      "59: vae loss: 19870.241455078125 - discr loss: 10.150415897369385\n",
      "60: vae loss: 23709.4951171875 - discr loss: 9.608367443084717\n",
      "61: vae loss: 13596.62744140625 - discr loss: 9.12504529953003\n",
      "62: vae loss: 19137.944091796875 - discr loss: 9.184349536895752\n",
      "63: vae loss: 18478.3388671875 - discr loss: 9.038318157196045\n",
      "64: vae loss: 25085.508056640625 - discr loss: 8.364006280899048\n",
      "65: vae loss: 21526.43701171875 - discr loss: 8.158631801605225\n",
      "66: vae loss: 14104.42578125 - discr loss: 7.643951416015625\n",
      "67: vae loss: 19898.52587890625 - discr loss: 9.830760717391968\n",
      "68: vae loss: 18264.542724609375 - discr loss: 7.676711678504944\n",
      "69: vae loss: 24520.388671875 - discr loss: 8.432581186294556\n",
      "70: vae loss: 17424.685546875 - discr loss: 8.06312382221222\n",
      "71: vae loss: 22129.542236328125 - discr loss: 8.262123227119446\n",
      "72: vae loss: 19804.289306640625 - discr loss: 6.3536447286605835\n",
      "73: vae loss: 24944.39013671875 - discr loss: 7.096001505851746\n",
      "74: vae loss: 14093.242431640625 - discr loss: 6.940798401832581\n",
      "75: vae loss: 14943.055908203125 - discr loss: 7.107724666595459\n",
      "76: vae loss: 13521.879150390625 - discr loss: 5.82106876373291\n",
      "77: vae loss: 16942.041748046875 - discr loss: 5.307260870933533\n",
      "78: vae loss: 20244.813720703125 - discr loss: 7.071468234062195\n",
      "79: vae loss: 22796.3505859375 - discr loss: 5.766578137874603\n",
      "80: vae loss: 16826.581298828125 - discr loss: 4.416423678398132\n",
      "81: vae loss: 17358.985595703125 - discr loss: 4.375902771949768\n",
      "82: vae loss: 15997.954833984375 - discr loss: 7.360135197639465\n",
      "83: vae loss: 13594.8369140625 - discr loss: 7.292439579963684\n",
      "84: vae loss: 15842.714599609375 - discr loss: 5.956250190734863\n",
      "85: vae loss: 14922.296142578125 - discr loss: 8.137444376945496\n",
      "86: vae loss: 17794.81494140625 - discr loss: 13.550954341888428\n",
      "87: vae loss: 9677.361694335938 - discr loss: 12.905205130577087\n",
      "88: vae loss: 14772.622680664062 - discr loss: 7.952568411827087\n",
      "89: vae loss: 21954.88037109375 - discr loss: 7.617845773696899\n",
      "90: vae loss: 19823.653076171875 - discr loss: 6.149344086647034\n",
      "91: vae loss: 18904.2607421875 - discr loss: 6.527319550514221\n",
      "92: vae loss: 11893.823852539062 - discr loss: 6.8924055099487305\n",
      "93: vae loss: 12481.89990234375 - discr loss: 6.607185482978821\n",
      "94: vae loss: 13268.017822265625 - discr loss: 5.621472120285034\n",
      "95: vae loss: 16009.832763671875 - discr loss: 6.318302392959595\n",
      "96: vae loss: 22174.345458984375 - discr loss: 7.669936180114746\n",
      "97: vae loss: 17441.7001953125 - discr loss: 6.065530061721802\n",
      "98: vae loss: 20461.90185546875 - discr loss: 6.007979035377502\n",
      "99: vae loss: 16573.59716796875 - discr loss: 5.136138081550598\n",
      "100: vae loss: 13666.882080078125 - discr loss: 5.711365461349487\n",
      "100: saving to results\n",
      "100: saving model to results\n",
      "101: vae loss: 17295.319580078125 - discr loss: 8.23821747303009\n",
      "102: vae loss: 17802.849365234375 - discr loss: 4.9954283237457275\n",
      "103: vae loss: 19378.135009765625 - discr loss: 6.444154262542725\n",
      "104: vae loss: 15381.26513671875 - discr loss: 6.194709897041321\n",
      "105: vae loss: 12767.947265625 - discr loss: 6.607359051704407\n",
      "106: vae loss: 11326.0859375 - discr loss: 5.103774309158325\n",
      "107: vae loss: 13539.99658203125 - discr loss: 3.8039669394493103\n",
      "108: vae loss: 19611.41943359375 - discr loss: 6.964262545108795\n",
      "109: vae loss: 16227.955810546875 - discr loss: 5.782247543334961\n",
      "110: vae loss: 16127.4990234375 - discr loss: 4.5303884744644165\n",
      "111: vae loss: 20196.040771484375 - discr loss: 4.0376837849617\n",
      "112: vae loss: 15307.89990234375 - discr loss: 4.347806513309479\n",
      "113: vae loss: 14734.537719726562 - discr loss: 5.864982962608337\n",
      "114: vae loss: 17980.54443359375 - discr loss: 4.362738370895386\n",
      "115: vae loss: 18025.75048828125 - discr loss: 4.446857452392578\n",
      "116: vae loss: 14065.383666992188 - discr loss: 4.94465309381485\n",
      "117: vae loss: 14269.736083984375 - discr loss: 5.035127639770508\n",
      "118: vae loss: 12755.711181640625 - discr loss: 3.962549149990082\n",
      "119: vae loss: 15727.013549804688 - discr loss: 4.087258577346802\n",
      "120: vae loss: 18244.091064453125 - discr loss: 3.301401734352112\n",
      "121: vae loss: 11781.10498046875 - discr loss: 11.970696568489075\n",
      "122: vae loss: 18194.67041015625 - discr loss: 11.650115728378296\n",
      "123: vae loss: 19268.123046875 - discr loss: 6.975663185119629\n",
      "124: vae loss: 18627.601318359375 - discr loss: 7.498974084854126\n",
      "125: vae loss: 13195.014770507812 - discr loss: 12.016843795776367\n",
      "126: vae loss: 17922.9560546875 - discr loss: 7.9492552280426025\n",
      "127: vae loss: 14200.589111328125 - discr loss: 101.32685661315918\n",
      "128: vae loss: 19713.240234375 - discr loss: 17.175681591033936\n",
      "129: vae loss: 19215.09326171875 - discr loss: 14.418650388717651\n",
      "130: vae loss: 20602.64453125 - discr loss: 8.817681789398193\n",
      "131: vae loss: 20037.004638671875 - discr loss: 8.698693633079529\n",
      "132: vae loss: 14507.1357421875 - discr loss: 5.923317313194275\n",
      "133: vae loss: 16213.210205078125 - discr loss: 15.545496582984924\n",
      "134: vae loss: 18845.295166015625 - discr loss: 7.4767597913742065\n",
      "135: vae loss: 16239.898681640625 - discr loss: 4.987513184547424\n",
      "136: vae loss: 17944.350830078125 - discr loss: 7.134454131126404\n",
      "137: vae loss: 18168.614013671875 - discr loss: 4.581390976905823\n",
      "138: vae loss: 9627.317993164062 - discr loss: 3.8252931237220764\n",
      "139: vae loss: 14089.472900390625 - discr loss: 3.6594237089157104\n",
      "140: vae loss: 14894.116943359375 - discr loss: 6.052811503410339\n",
      "141: vae loss: 22200.072509765625 - discr loss: 5.205499827861786\n",
      "142: vae loss: 20915.0419921875 - discr loss: 4.692121744155884\n",
      "143: vae loss: 20860.703369140625 - discr loss: 4.501001715660095\n",
      "144: vae loss: 14258.70654296875 - discr loss: 3.1634358167648315\n",
      "145: vae loss: 11717.82275390625 - discr loss: 3.4310697317123413\n",
      "146: vae loss: 15647.067138671875 - discr loss: 3.7170644402503967\n",
      "147: vae loss: 17024.18212890625 - discr loss: 6.145950615406036\n",
      "148: vae loss: 15467.667602539062 - discr loss: 3.7271289825439453\n",
      "149: vae loss: 14140.910888671875 - discr loss: 3.6321420073509216\n",
      "150: vae loss: 17349.21728515625 - discr loss: 4.14637964963913\n",
      "151: vae loss: 19979.059326171875 - discr loss: 3.41591876745224\n",
      "152: vae loss: 15107.744873046875 - discr loss: 2.835827350616455\n",
      "153: vae loss: 11285.938110351562 - discr loss: 3.0409862399101257\n",
      "154: vae loss: 17238.769287109375 - discr loss: 1.5646947920322418\n",
      "155: vae loss: 18647.926513671875 - discr loss: 2.12135112285614\n",
      "156: vae loss: 14815.005615234375 - discr loss: 3.756313145160675\n",
      "157: vae loss: 22989.05712890625 - discr loss: 6.624662518501282\n",
      "158: vae loss: 15553.39892578125 - discr loss: 2.999451696872711\n",
      "159: vae loss: 17460.42138671875 - discr loss: 4.7039796113967896\n",
      "160: vae loss: 16122.63818359375 - discr loss: 2.5684030055999756\n",
      "161: vae loss: 23563.120361328125 - discr loss: 11.619959980249405\n",
      "162: vae loss: 14981.87890625 - discr loss: 2.1436140537261963\n",
      "163: vae loss: 13139.8408203125 - discr loss: 2.0587935745716095\n",
      "164: vae loss: 17474.714599609375 - discr loss: 4.225100755691528\n",
      "165: vae loss: 12950.215087890625 - discr loss: 4.0510082840919495\n",
      "166: vae loss: 18629.544921875 - discr loss: 6.852734923362732\n",
      "167: vae loss: 14922.042724609375 - discr loss: 2.8154517710208893\n",
      "168: vae loss: 16414.4833984375 - discr loss: 5.185077130794525\n",
      "169: vae loss: 12911.877197265625 - discr loss: 3.1860559582710266\n",
      "170: vae loss: 18722.367919921875 - discr loss: 2.7210851311683655\n",
      "171: vae loss: 18610.016845703125 - discr loss: 3.396078944206238\n",
      "172: vae loss: 22326.798095703125 - discr loss: 2.9876537322998047\n",
      "173: vae loss: 15393.215576171875 - discr loss: 3.4751749634742737\n",
      "174: vae loss: 14983.56591796875 - discr loss: 2.880325973033905\n",
      "175: vae loss: 19769.078857421875 - discr loss: 2.831518828868866\n",
      "176: vae loss: 13559.065185546875 - discr loss: 2.1258577704429626\n",
      "177: vae loss: 18046.513916015625 - discr loss: 3.153123915195465\n",
      "178: vae loss: 16463.555419921875 - discr loss: 1.86859530210495\n",
      "179: vae loss: 12048.463623046875 - discr loss: 1.8096564710140228\n",
      "180: vae loss: 18695.94384765625 - discr loss: 1.526757150888443\n",
      "181: vae loss: 9152.41357421875 - discr loss: 1.980182260274887\n",
      "182: vae loss: 15440.267333984375 - discr loss: 1.8500947952270508\n",
      "183: vae loss: 16777.3369140625 - discr loss: 0.8654072731733322\n",
      "184: vae loss: 15652.155029296875 - discr loss: 0.9688623212277889\n",
      "185: vae loss: 8890.93115234375 - discr loss: 0.5712976958602667\n",
      "186: vae loss: 13230.56103515625 - discr loss: 1.191315658390522\n",
      "187: vae loss: 15863.876708984375 - discr loss: 0.28462234884500504\n",
      "188: vae loss: 10634.133056640625 - discr loss: 1.1110081896185875\n",
      "189: vae loss: 16217.0849609375 - discr loss: 0.7922468334436417\n",
      "190: vae loss: 9689.691772460938 - discr loss: 3.1536551415920258\n",
      "191: vae loss: 17555.195556640625 - discr loss: 0.46309520304203033\n",
      "192: vae loss: 14079.876220703125 - discr loss: 0.18859411403536797\n",
      "193: vae loss: 14800.78662109375 - discr loss: 0.6374081606045365\n",
      "194: vae loss: 16161.64453125 - discr loss: 0.3660278171300888\n",
      "195: vae loss: 15690.749755859375 - discr loss: 0.7991222441196442\n",
      "196: vae loss: 19280.925537109375 - discr loss: 0.3626698851585388\n",
      "197: vae loss: 17717.643310546875 - discr loss: 1.597235031425953\n",
      "198: vae loss: 15425.962646484375 - discr loss: 0.3734888844192028\n",
      "199: vae loss: 14150.73095703125 - discr loss: 0.5911797881126404\n",
      "200: vae loss: 15168.818115234375 - discr loss: 0.5288593769073486\n",
      "200: saving to results\n",
      "200: saving model to results\n",
      "201: vae loss: 16353.513427734375 - discr loss: 0.44097617268562317\n",
      "202: vae loss: 14833.53857421875 - discr loss: 0.3085663914680481\n",
      "203: vae loss: 13461.48974609375 - discr loss: 1.5267051495611668\n",
      "204: vae loss: 16423.725952148438 - discr loss: 0.9072883948683739\n",
      "205: vae loss: 13359.298095703125 - discr loss: 0.26316629722714424\n"
     ]
    }
   ],
   "source": [
    "trainer.train()               # reconstructions and checkpoints will be saved periodically to ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Phenaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phenaki_pytorch import Phenaki, PhenakiTrainer\n",
    "\n",
    "phenaki = Phenaki(\n",
    "    cvivit = cvivit,\n",
    "    self_token_critic= True  # set this to True\n",
    ").to(device)\n",
    "phenaki_trainer = PhenakiTrainer(\n",
    "    phenaki,\n",
    "    batch_size=4,\n",
    "    num_frames=17,\n",
    "    train_lr=0.0001,\n",
    "    train_num_steps=2,\n",
    "    grad_accum_every = 2,\n",
    "    train_on_images=False,\n",
    "    save_and_sample_every=100,\n",
    "    num_samples=4,\n",
    "    dataset = train_dataset,\n",
    "    sample_texts_file_path = f\"{'/content' if IN_COLAB else '/home/luthando/Desktop'}/phenaki-pytorch/data/sample_texts.txt\" # each caption should be on a new line, during sampling, will be randomly drawn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenaki_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = phenaki.sample(texts = 'a squirrel examines an acorn', num_frames = 17, cond_scale = 5.) # (1, 3, 17, 256, 128)\n",
    "\n",
    "# # so in the paper, they do not really achieve 2 minutes of coherent video\n",
    "# # at each new scene with new text conditioning, they condition on the previous K frames\n",
    "# # you can easily achieve this with this framework as so\n",
    "\n",
    "# video_prime = video[:, :, -3:] # (1, 3, 3, 256, 128) # say K = 3\n",
    "# video_next = phenaki.sample(texts = 'a cat watches the squirrel from afar', prime_frames = video_prime, num_frames = 14) # (1, 3, 14, 256, 128)\n",
    "\n",
    "# # the total video\n",
    "# entire_video = torch.cat((video, video_next), dim = 2) # (1, 3, 17 + 14, 256, 128)\n",
    "\n",
    "# # and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ... above code\n",
    "\n",
    "# from phenaki_pytorch import make_video\n",
    "\n",
    "# entire_video, scenes = make_video(phenaki, texts = [\n",
    "#     'a squirrel examines an acorn buried in the snow',\n",
    "#     'a cat watches the squirrel from a frosted window sill',\n",
    "#     'zoom out to show the entire living room, with the cat residing by the window sill'\n",
    "# ], num_frames = (17, 14, 14), prime_lengths = (5, 5))\n",
    "\n",
    "# entire_video.shape # (1, 3, 17 + 14 + 14 = 45, 256, 256)\n",
    "\n",
    "# # scenes - List[Tensor[3]] - video segment of each scene"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
