{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/phenaki-pytorch/blob/luthando-contribution/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, WORKING_DIR)\n",
    "else:\n",
    "    # The actual code is one level higher in folder depth/structure, so we're elevating this notebook.\n",
    "    sys.path.insert(0,f\".{WORKING_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/LuthandoMaqondo/phenaki-pytorch.git\n",
    "    %cd /content/phenaki-pytorch\n",
    "    !git checkout luthando-contribution\n",
    "    !pip install -r requirements.txt\n",
    "    # !pip install phenaki-pytorch\n",
    "    !pip install git+https://xxxxxxxxxxxx@github.com/AppimateSA/AutoVisual.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'dataset.json' (from Source)\n",
      "Downloading 'vocab.txt' (from Source)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading videos (from Source) : 100%|\u001b[38;2;0;255;255m████████████████████████████\u001b[0m| 34/34 [00:39<00:00,  1.17s/it]\u001b[0m\n",
      "100%|██████████| 3/3 [00:20<00:00,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset prepared, 'dataset.json' file gives us: 34 text & 34 video datapoints.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from autovisual import DatasetConfig, VideoCustomDataset\n",
    "dataset_config_args = {\n",
    "    'refreshData': False,\n",
    "    # 'useCLIP': \"openai/clip-vit-large-patch14\", # Use the pretrained CLIP model for handling ALL Text inputs.\n",
    "    'structure': 'text_video_pair',\n",
    "    'tokenize_text': False,\n",
    "    # 'data_folder': f'.{WORKING_DIR}/datasets/Appimate',\n",
    "    'data_folder': 'https://appimate1storage.blob.core.windows.net/datasets/Appimate',\n",
    "    'data_json': \"dataset.json\",\n",
    "    'data_points': None, # None\n",
    "\n",
    "    'max_text_length': 77,\n",
    "    'max_num_frames': 6,\n",
    "    'resolution': 64,\n",
    "    'num_channels': 1, \n",
    "    'normalize': True,\n",
    "    'scale_to': 0,#0.5,\n",
    "    'has_start_end_token': True,\n",
    "\n",
    "    'frame_rate': 2,\n",
    "    'frame_rate_ratio': 0.01,\n",
    "    'output_format': 'TCHW'\n",
    "}\n",
    "datasetConfig = DatasetConfig(**dataset_config_args, train=True)\n",
    "custom_dataset = VideoCustomDataset(datasetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockTextVideoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        length = 100,\n",
    "        image_size = 256,\n",
    "        num_frames = 17\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.len = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = torch.randn(3, self.num_frames, self.image_size, self.image_size)\n",
    "        caption = 'video caption'\n",
    "        return video, caption\n",
    "\n",
    "mock_dataset = MockTextVideoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ConcatDataset([\n",
    "    mock_dataset,\n",
    "    # custom_dataset\n",
    "])\n",
    "\n",
    "train_len = int(len(full_dataset) * (datasetConfig.train_split) )\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_len, len(full_dataset)- train_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     # text_in, visuals_out = batch\n",
    "#     # print('text_in: ', text_in)\n",
    "#     # print('visuals_out: ', visuals_out)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the C-ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 57 samples and validating with randomly splitted 4 samples\n"
     ]
    }
   ],
   "source": [
    "from phenaki_pytorch import CViViT, CViViTTrainer, MaskGit, Phenaki, PhenakiTrainer\n",
    "\n",
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = (256, 256),\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").to(device)\n",
    "\n",
    "data_folder = os.path.expanduser(f\"{WORKING_DIR}/datasets/Appimate/train\") if IN_COLAB else os.path.expanduser(f\"~/.cache/datasets/Appimate/train\")\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder = data_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 4,\n",
    "    train_on_images = False,  # you can train on images first, before fine tuning on video, for sample efficiency\n",
    "    use_ema = True,          # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "    num_train_steps = 10\n",
    ")\n",
    "# trainer.train()               # reconstructions and checkpoints will be saved periodically to ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Phenaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskgit = MaskGit(\n",
    "    num_tokens = 5000,\n",
    "    max_seq_len = 1024,\n",
    "    dim = 512,\n",
    "    dim_context = 768,\n",
    "    depth = 6,\n",
    ").to(device)\n",
    "\n",
    "phenaki = Phenaki(\n",
    "    cvivit = cvivit,\n",
    "    maskgit = maskgit\n",
    ").to(device)\n",
    "phenaki_trainer = PhenakiTrainer(\n",
    "    phenaki,\n",
    "    batch_size=4,\n",
    "    num_frames=17,\n",
    "    train_lr=0.0001,\n",
    "    train_num_steps=2,\n",
    "    grad_accum_every = 2,\n",
    "    train_on_images=False,\n",
    "    save_and_sample_every=100,\n",
    "    num_samples=4,\n",
    "    dataset = train_dataset,\n",
    "    # sample_texts_file_path = f\"{'/content'}/phenaki-pytorch/data/sample_texts.txt\" # each caption should be on a new line, during sampling, will be randomly drawn\n",
    "    sample_texts_file_path = f\"/Users/luthandomaqondo/Development/Python/phenaki-pytorch/data/sample_texts.txt\" # each caption should be on a new line, during sampling, will be randomly drawn\n",
    ")\n",
    "phenaki_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = phenaki.sample(texts = 'a squirrel examines an acorn', num_frames = 17, cond_scale = 5.) # (1, 3, 17, 256, 128)\n",
    "\n",
    "# so in the paper, they do not really achieve 2 minutes of coherent video\n",
    "# at each new scene with new text conditioning, they condition on the previous K frames\n",
    "# you can easily achieve this with this framework as so\n",
    "\n",
    "video_prime = video[:, :, -3:] # (1, 3, 3, 256, 128) # say K = 3\n",
    "video_next = phenaki.sample(texts = 'a cat watches the squirrel from afar', prime_frames = video_prime, num_frames = 14) # (1, 3, 14, 256, 128)\n",
    "\n",
    "# the total video\n",
    "entire_video = torch.cat((video, video_next), dim = 2) # (1, 3, 17 + 14, 256, 128)\n",
    "\n",
    "# and so on..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
