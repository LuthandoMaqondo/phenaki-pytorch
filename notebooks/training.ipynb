{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "WORKING_DIR = \"../\"\n",
    "sys.path.insert(0, WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install phenaki-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from phenaki_pytorch import CViViT, CViViTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luthandomaqondo/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/luthandomaqondo/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/luthandomaqondo/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      " 45%|████▌     | 238M/528M [06:10<10:40, 474kB/s]    "
     ]
    }
   ],
   "source": [
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "data_folder = os.path.expanduser(f\"~/.cache/Appimate\")\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder = data_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 4,\n",
    "    train_on_images = False,  # you can train on images first, before fine tuning on video, for sample efficiency\n",
    "    use_ema = False,          # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "    num_train_steps = 10000\n",
    ")\n",
    "\n",
    "# trainer.train()               # reconstructions and checkpoints will be saved periodically to ./results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from phenaki_pytorch import CViViT, MaskGit, Phenaki\n",
    "\n",
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = (256, 128),  # video with rectangular screen allowed\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "model_path = os.path.expanduser(f\"~/.cache/Appimate\")\n",
    "cvivit.load(model_path)\n",
    "# cvivit.load('/path/to/trained/cvivit.pt')\n",
    "maskgit = MaskGit(\n",
    "    num_tokens = 5000,\n",
    "    max_seq_len = 1024,\n",
    "    dim = 512,\n",
    "    dim_context = 768,\n",
    "    depth = 6,\n",
    ")\n",
    "\n",
    "phenaki = Phenaki(\n",
    "    cvivit = cvivit,\n",
    "    maskgit = maskgit\n",
    ").cuda()\n",
    "\n",
    "videos = torch.randn(3, 3, 17, 256, 128).cuda() # (batch, channels, frames, height, width)\n",
    "mask = torch.ones((3, 17)).bool().cuda() # [optional] (batch, frames) - allows for co-training videos of different lengths as well as video and images in the same batch\n",
    "\n",
    "texts = [\n",
    "    'a whale breaching from afar',\n",
    "    'young girl blowing out candles on her birthday cake',\n",
    "    'fireworks with blue and green sparkles'\n",
    "]\n",
    "\n",
    "loss = phenaki(videos, texts = texts, video_frame_mask = mask)\n",
    "loss.backward()\n",
    "\n",
    "# do the above for many steps, then ...\n",
    "\n",
    "video = phenaki.sample(texts = 'a squirrel examines an acorn', num_frames = 17, cond_scale = 5.) # (1, 3, 17, 256, 128)\n",
    "\n",
    "# so in the paper, they do not really achieve 2 minutes of coherent video\n",
    "# at each new scene with new text conditioning, they condition on the previous K frames\n",
    "# you can easily achieve this with this framework as so\n",
    "\n",
    "video_prime = video[:, :, -3:] # (1, 3, 3, 256, 128) # say K = 3\n",
    "\n",
    "video_next = phenaki.sample(texts = 'a cat watches the squirrel from afar', prime_frames = video_prime, num_frames = 14) # (1, 3, 14, 256, 128)\n",
    "\n",
    "# the total video\n",
    "\n",
    "entire_video = torch.cat((video, video_next), dim = 2) # (1, 3, 17 + 14, 256, 128)\n",
    "\n",
    "# and so on..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
