{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/phenaki-pytorch/blob/main/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)\n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, WORKING_DIR)\n",
    "else:\n",
    "    # The actual code is one level higher in folder depth/structure, so we're elevating this notebook.\n",
    "    sys.path.insert(0,f\".{WORKING_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phenaki-pytorch\n",
    "!pip install git+https://xxxxxxxxxxxx@github.com/AppimateSA/AutoVisual.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovisual import DatasetConfig, VideoCustomDataset\n",
    "dataset_config_args = {\n",
    "    'refreshData': True,\n",
    "    # 'useCLIP': \"openai/clip-vit-large-patch14\", # Use the pretrained CLIP model for handling ALL Text inputs.\n",
    "    'structure': 'text_video_pair',\n",
    "    # 'data_folder': f'.{WORKING_DIR}/datasets/Appimate',\n",
    "    'data_folder': 'https://appimate1storage.blob.core.windows.net/datasets/Appimate',\n",
    "    'data_json': \"dataset.json\",\n",
    "    'data_points': None, # None\n",
    "\n",
    "    'max_text_length': 77,\n",
    "    'max_num_frames': 6,\n",
    "    'resolution': 64,\n",
    "    'num_channels': 1, \n",
    "    'normalize': True,\n",
    "    'scale_to': 0,#0.5,\n",
    "    'has_start_end_token': True,\n",
    "\n",
    "    'frame_rate': 2,\n",
    "    'frame_rate_ratio': 0.01,\n",
    "    'output_format': 'TCHW'\n",
    "}\n",
    "datasetConfig = DatasetConfig(**dataset_config_args, train=True)\n",
    "full_dataset = VideoCustomDataset(datasetConfig)\n",
    "train_len = int(len(full_dataset) * (datasetConfig.train_split) )\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_len, len(full_dataset)- train_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the C-ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phenaki_pytorch import CViViT, CViViTTrainer, MaskGit, Phenaki, PhenakiTrainer\n",
    "\n",
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = (256, 256),\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").cuda()\n",
    "\n",
    "data_folder = os.path.expanduser(f\"{WORKING_DIR}/datasets/Appimate/train\") if IN_COLAB else os.path.expanduser(f\"~/.cache/datasets/Appimate/train\")\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder = data_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 4,\n",
    "    train_on_images = False,  # you can train on images first, before fine tuning on video, for sample efficiency\n",
    "    use_ema = True,          # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "    num_train_steps = 10\n",
    ")\n",
    "trainer.train()               # reconstructions and checkpoints will be saved periodically to ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Phenaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maskgit = MaskGit(\n",
    "    num_tokens = 5000,\n",
    "    max_seq_len = 1024,\n",
    "    dim = 512,\n",
    "    dim_context = 768,\n",
    "    depth = 6,\n",
    ")\n",
    "\n",
    "phenaki = Phenaki(\n",
    "    cvivit = cvivit,\n",
    "    maskgit = maskgit\n",
    ").cuda()\n",
    "phenaki_trainer = PhenakiTrainer(\n",
    "    phenaki,\n",
    "    batch_size=4,\n",
    "    num_frames=17,\n",
    "    train_lr=0.0001,\n",
    "    train_num_steps=1000,\n",
    "    train_on_images=False,\n",
    "    save_and_sample_every=100,\n",
    "    num_samples=4,\n",
    "    dataset = train_dataset,\n",
    "    sample_texts_file_path = f\"{'/content'}/phenaki-pytorch/data/sample_texts.txt\" # each caption should be on a new line, during sampling, will be randomly drawn\n",
    ")\n",
    "phenaki_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = phenaki.sample(texts = 'a squirrel examines an acorn', num_frames = 17, cond_scale = 5.) # (1, 3, 17, 256, 128)\n",
    "\n",
    "# so in the paper, they do not really achieve 2 minutes of coherent video\n",
    "# at each new scene with new text conditioning, they condition on the previous K frames\n",
    "# you can easily achieve this with this framework as so\n",
    "\n",
    "video_prime = video[:, :, -3:] # (1, 3, 3, 256, 128) # say K = 3\n",
    "video_next = phenaki.sample(texts = 'a cat watches the squirrel from afar', prime_frames = video_prime, num_frames = 14) # (1, 3, 14, 256, 128)\n",
    "\n",
    "# the total video\n",
    "entire_video = torch.cat((video, video_next), dim = 2) # (1, 3, 17 + 14, 256, 128)\n",
    "\n",
    "# and so on..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
